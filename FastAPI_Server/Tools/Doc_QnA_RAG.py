# import os
# import time
# from langchain.tools import tool
# from langchain_text_splitters import RecursiveCharacterTextSplitter

# # from langchain_huggingface import HuggingFaceEndpointEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings
# import sentence_transformers
# from langchain_community.vectorstores import FAISS
# from langchain.chains import RetrievalQA
# from langchain_core.prompts import PromptTemplate

# # from langchain_core.runnables import RunnableParallel, RunnablePassthrough
# from langchain_google_genai import ChatGoogleGenerativeAI
# from unstract.llmwhisperer import LLMWhispererClientV2
# import hashlib


# # Environment setup
# from dotenv import load_dotenv

# load_dotenv()


# def get_file_hash(file_path: str) -> str:
#     """Compute MD5 hash of the file content."""
#     hasher = hashlib.md5()
#     with open(file_path, "rb") as f:
#         while chunk := f.read(8192):
#             hasher.update(chunk)
#     return hasher.hexdigest()


# embeddings_model = HuggingFaceEmbeddings(
#     model_name="sentence-transformers/static-retrieval-mrl-en-v1"
# )

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)


# model = ChatGoogleGenerativeAI(
#     model="gemini-2.0-flash", temperature=0, google_api_key=os.getenv("GOOGLE_API_KEY")
# )


# # LLM Whisperer client setup
# llm_whisperer = LLMWhispererClientV2(
#     base_url="https://llmwhisperer-api.us-central.unstract.com/api/v2",
#     api_key=os.getenv("LLM_WHISPERER_API_KEY"),
# )

# prompt = PromptTemplate(
#     input_variables=["context", "question"],
#     template="""
#     You are a highly knowledgeable financial assistant. Use only the information provided in the context below to answer the user's question. Do not use prior knowledge.
#     Use all the context that is given to you the best you can and give the answer.
#     If the context does not contain the information needed, respond with:
#     "The answer is not available in the provided document."
#     Use the following context to answer the question.
#     ---
#     Context:
#     {context}
#     ---
#     Question: {question}

#     Answer:
#     """,
# )


# def pdf_to_text(file_path: str) -> str:
#     """Extract text from PDF using LLM Whisperer"""
#     result = llm_whisperer.whisper(file_path=file_path)

#     while True:
#         status = llm_whisperer.whisper_status(whisper_hash=result["whisper_hash"])
#         if status["status"] == "processed":
#             result = llm_whisperer.whisper_retrieve(whisper_hash=result["whisper_hash"])
#             return result["extraction"]["result_text"]
#         time.sleep(5)


# rag_chain = None

# vector_store = None

# CACHE_DIR = "./faiss_cache"
# os.makedirs(CACHE_DIR, exist_ok=True)


# def setup_rag_system(file_path: str):
#     """Process PDF and create FAISS vector store"""
#     global vector_store

#     file_hash = get_file_hash(file_path)
#     cache_path = os.path.join(CACHE_DIR, file_hash)

#     # Try loading cached FAISS index
#     if os.path.exists(cache_path):
#         try:
#             print(f"Loading cached FAISS index for file hash {file_hash}...")
#             vector_store = FAISS.load_local(
#                 cache_path, embeddings_model, allow_dangerous_deserialization=True
#             )
#             print("Loaded cached vector store successfully.")
#             return vector_store
#         except Exception as e:
#             print(f"Failed to load cache: {e}. Reprocessing...")

#     # Extract text from PDF
#     extracted_text = pdf_to_text(file_path)
#     print("Text Extracted....")

#     # Split text
#     chunks = text_splitter.split_text(extracted_text)
#     print("Chunks created....\n")

#     metadatas = [
#         {"source": os.path.basename(file_path), "file_hash": file_hash} for _ in chunks
#     ]

#     # Create FAISS index
#     vector_store = FAISS.from_texts(texts=chunks, embedding=embeddings_model)
#     vector_store.save_local(cache_path)
#     print(f"Saved FAISS index cache at {cache_path}")

#     return vector_store


# @tool
# def rag_qa_tool(file_path: str, query: str) -> str:
#     """ "
#     Query the processed document using the FAISS-based RAG system.

#     Args:
#         query (str): The user's question to be answered based on the processed document.

#     Returns:
#         str: The answer generated by the RAG system, or an error message if no document
#              has been processed or if an error occurs during processing.

#     Description:
#         This function checks if a document has been processed and a vector store exists.
#         It then invokes the RAG chain to retrieve relevant context and generate an answer
#         to the user's query.
#     """

#     vector_store = setup_rag_system(file_path=file_path)

#     if not vector_store:
#         return "No documents processed. Process a PDF first."
#     rag_chain = RetrievalQA.from_chain_type(
#         llm=model,
#         retriever=vector_store.as_retriever(search_kwargs={"k": 20}),
#         chain_type="stuff",
#         return_source_documents=True,
#     )

#     try:
#         return rag_chain.invoke(query)["result"]
#     except Exception as e:
#         return f"Error processing query: {str(e)}"


# # print(rag_qa_tool.invoke({
# #     "file_path": "/home/saikrishnanair/Finance-GPT/2PageNvidia.pdf",
# #     "query": "How much is the total revenue for the year 2024?"
# # }))


import os
import time
from langchain.tools import tool
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List, Union
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, SystemMessage

# from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
import sentence_transformers
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI
from unstract.llmwhisperer import LLMWhispererClientV2
import hashlib


# Environment setup
from dotenv import load_dotenv

load_dotenv()


def get_file_hash(file_path: str) -> str:
    """Compute MD5 hash of the file content."""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        while chunk := f.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()


embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/static-retrieval-mrl-en-v1"
)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)


model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash", temperature=0, google_api_key=os.getenv("GOOGLE_API_KEY")
)


# LLM Whisperer client setup
llm_whisperer = LLMWhispererClientV2(
    base_url="https://llmwhisperer-api.us-central.unstract.com/api/v2",
    api_key=os.getenv("LLM_WHISPERER_API_KEY"),
)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a financial document analyst. Use only the information provided in the context below to answer the user's question. Do not use prior knowledge.

Guidelines:
1. Keep responses under 150 words and focused
2. Include specific numbers, dates, and financial metrics when available
3. Be precise and professional
4. If the question includes context from previous analysis, use that understanding

If the context does not contain the information needed, respond with:
"The answer is not available in the provided document."

Context:
{context}

Question: {question}

Answer:
""",
)


def pdf_to_text(file_path: str) -> str:
    """Extract text from PDF using LLM Whisperer"""
    result = llm_whisperer.whisper(file_path=file_path)

    while True:
        status = llm_whisperer.whisper_status(whisper_hash=result["whisper_hash"])
        if status["status"] == "processed":
            result = llm_whisperer.whisper_retrieve(whisper_hash=result["whisper_hash"])
            return result["extraction"]["result_text"]
        time.sleep(5)


rag_chain = None

vector_store = None

CACHE_DIR = "./faiss_cache"
os.makedirs(CACHE_DIR, exist_ok=True)


def setup_rag_system(file_path: str):
    """Process PDF and create FAISS vector store"""
    global vector_store

    file_hash = get_file_hash(file_path)
    cache_path = os.path.join(CACHE_DIR, file_hash)

    # Try loading cached FAISS index
    if os.path.exists(cache_path):
        try:
            print(f"Loading cached FAISS index for file hash {file_hash}...")
            vector_store = FAISS.load_local(
                cache_path, embeddings_model, allow_dangerous_deserialization=True
            )
            print("Loaded cached vector store successfully.")
            return vector_store
        except Exception as e:
            print(f"Failed to load cache: {e}. Reprocessing...")

    # Extract text from PDF
    extracted_text = pdf_to_text(file_path)
    print("Text Extracted....")

    # Split text
    chunks = text_splitter.split_text(extracted_text)
    print("Chunks created....\n")

    metadatas = [
        {"source": os.path.basename(file_path), "file_hash": file_hash} for _ in chunks
    ]

    # Create FAISS index
    vector_store = FAISS.from_texts(texts=chunks, embedding=embeddings_model)
    vector_store.save_local(cache_path)
    print(f"Saved FAISS index cache at {cache_path}")

    return vector_store


@tool
def rag_qa_tool(
    file_path: str,
    query: str,
    dependency_context: str = "",
    message_history: List[Union[AIMessage, HumanMessage]] = [],
) -> str:
    """
    Query the processed document using the FAISS-based RAG system with full context.
    Accepts dependency context and prior message history to enable multi-agent reasoning.
    """

    print(f"Original Query: {query}")
    print(f"Dependency Context: {dependency_context}")

    def format_history(history: List[BaseMessage]) -> str:
        """Format message history into a string for model input."""
        formatted = ""
        for msg in history[-10:]:
            role = "User" if isinstance(msg, HumanMessage) else "Assistant"
            formatted += f"{role}: {msg.content}\n"
        return formatted.strip()

    history_str = format_history(message_history)

    full_input = f"""You are analyzing a PDF document as part of a multi-agent pipeline.

    Original User Query:
    {query}

    --- Dependency Context ---
    {dependency_context}

    --- Prior History ---
    {history_str}
    """

    query_parsing_prompt = """You are an expert financial document analyst. Your task is to analyze the given input (which may contain both a query , context from previous analysis , previous message history) and extract the core question about the document.

#     The input may be in formats like:
#     - "How much is the total revenue for the year 2024?"
#     - "profit analysis based on: previous step identified focus on Q3 performance metrics"
#     - "balance sheet ratios context: looking for liquidity analysis from financial news"

#     Guidelines:
#     1. Parse the input to identify the main question and any contextual information
#     2. Extract the core question that needs to be answered about the document
#     3. Identify specific financial metrics, ratios, or analysis areas mentioned
#     4. Keep context in mind but focus on what specific information is being requested
#     5. Be concise and direct

#     Output only the refined question - no explanations or additional text."""

    try:
        query_parsing_messages = [
            SystemMessage(content=query_parsing_prompt),
            HumanMessage(content=full_input),
        ]

        refined_query = model.invoke(query_parsing_messages).content.strip()
        print(f"\nRefined query: {refined_query}")

        vector_store = setup_rag_system(file_path=file_path)

        if not vector_store:
            return "Document processing failed. Please upload a valid document."

        rag_chain = RetrievalQA.from_chain_type(
            llm=model,
            retriever=vector_store.as_retriever(search_kwargs={"k": 20}),
            chain_type="stuff",
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt},
        )

        context_aware_query = f"""Refined Question: {refined_query}
            Original Query: {query}
            Dependency Summary: {dependency_context}
            Chat History Summary: {history_str}
            Please provide a detailed 150-word maximum answer using document information only. Include dates, metrics, and be precise.
"""

        result = rag_chain.invoke({"query": context_aware_query})
        return result["result"]

    except Exception as e:
        return f"[ERROR] RAG query processing failed: {str(e)}"


# Example usage with concatenated context
# print(
#     rag_qa_tool.invoke(
#         {
#             "file_path": "/home/saikrishnanair/Downloads/2PageNvidia.pdf",
#             "query": "How much is the total revenue for the year 2024?",
#             "dependency_context": "",
#             "message_history": [],
#         }
#     )
# )

# Basic usage
# print(rag_qa_tool.invoke({
#     "file_path": "/home/saikrishnanair/Finance-GPT/2PageNvidia.pdf",
#     "query": "How much is the total revenue for the year 2024?"
# }))
